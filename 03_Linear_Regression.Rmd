---
title: "03_Linear Regression"
author: "Lian Rui"
date: "2024-07-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = F,
                      warning = F)
```

```{r}
library(tidyverse)
```

## Simple Linear Regression

Assume a linear relationship: 

$$
Y = \beta_0 + \beta_1X + \epsilon \approx \beta_0 + \beta_1X
$$

Estimate the linear relationship: 

$$
\hat{y} = \hat{\beta}_{0} + \hat{\beta_1}x
$$

### 3.1.1 Estimating the Coefficients

Residual sum of squares (RSS)

$$
\begin{align}
\text{RSS} &= e_1^2 + e_2^2 + \cdots + e_n^{2} \\
&= (y_1 - \hat{\beta}_0^2 - \hat{\beta}_1x_1)^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_2x_2)^2 + \cdots + (y_1 - \hat{\beta}_0^2 - \hat{\beta}_nx_n)^2
\end{align}
$$

### 3.1.2 Assessing the Accuracy of the Coefficient Estimates

__Simulation of fig 3.3 to show population line and least square line__

Summary: 
Population regression line, least squares line, bias, unbiased, standard error of estimate, Residual standard error, confident interval hypothesis test, null hypothesis, alternative hypothesis, t-statistic. 

__Background:__ 
Model assumptions and factors that lead to linear regression model unreliable 

-. Linearity: linear relationship between outcome and predictors. 

-. Independence: no collinearity among predictors. 

-. Homoscedasticity: variance of residuals is constant. 

-. Normal Residuals: residuals are normal distributed. 

  - Residual plot to check linearity, homoscedasticity and identify outliers
  
  - Histograms or QQ plot of residuals to check normality
  
  - Variance Inflation Factor (VIF) to check multicollinearity
  
  - Durbin-Watson test for check for autocorrelation. 

-. No Autocorrelations: 

-. Sample size: 

-. No outliers. 

-. Model is appropriately specified. 

Population line is theoretical, and we always get the least square line. 

Standard error of the estimates of linear regression: 

__Standard Errors of Estimates__

Standard error of $\hat{\beta_0}$: 

$$
\text{SE}(\hat{\beta}_0) = \sqrt{\sigma^2 \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2} \right)}
$$

Standard error of $\hat{\beta_1}$:

$$
\text{SE}(\hat{\beta}_1) = \sqrt{\frac{\sigma^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}}

$$

all the uncertainties or unreliability of estimates are in sigma square. And sigma square may have something not random is the assumption is violated. 

__Hypothesis Testing__

$H_0$: There is no relationship between X and Y, $H_0: \beta_1 = 0$

$H_a$: There is some relationship between X and Y, $H_1: \beta1 \neq 0$

t-statistic: 

$$
t = \frac{\hat{\beta_1} - 0}{\text{SE}(\hat{\beta_1})}
$$

### 3.1.3 Assessing the Accuracy of the Model

__Three Metrics__: 

RSE: average size of residuals, how well the model fit the data in absolute values. 

$$
\text{RSE} = \sqrt{\frac{1}{n-2}\text{RSS}} = \sqrt{\frac{1}{n-2}\sum_{i = 1}^{n}(y_i -\hat{y_i}^2)}
$$

$R^2$: The proportion of the variance in the outcome variable that is explained by the model, goodness of fit. 

$$
\text{TSS} = \sum(y_i - \bar{y})^2
$$

$$
R^2 = \frac{\text{TSS} - \text{RSS}}{TSS}
$$

F-statistic: Overall significance of the model, whether the predictors, as a group, contribute significantly to explain the outcome variable. 

$$
\begin{align}
F &= \frac{(\text{TSS}- \text{RSS})/p}{\text{RSS}/(n - p -1)} \\
&= \frac{\text{MS}_\text{reg}}{\text{MS}_\text{rss}}
\end{align}
$$

__Three Metrics__ may have discrepancies. 

Yes, it's possible for the Residual Standard Error (RSE), R-squared (\(R^2\)), and F-statistic to present different or even conflicting indications about a linear regression model's performance. Here's how these metrics can sometimes go in different directions:

### 1. **RSE and \(R^2\)**

- **RSE and \(R^2\) Relationship**:
  - Generally, \(R^2\) and RSE are inversely related. A higher \(R^2\) indicates that a larger proportion of the variance is explained by the model, which typically results in a lower RSE, as the residuals are smaller on average.
  - However, this is not always straightforward. Adding more predictors to a model can increase \(R^2\) (even if the predictors are not truly informative), while RSE might not decrease significantly or could even increase if the additional predictors do not genuinely improve the model.

- **Possible Discrepancies**:
  - **High \(R^2\) with High RSE**: If \(R^2\) is high but RSE is still large, it could mean that the model explains a good portion of variance, but there is still a lot of variability in the residuals, possibly due to outliers or variability not captured by the model.
  - **Low \(R^2\) with Low RSE**: If \(R^2\) is low but RSE is low as well, the model might fit the data well in an absolute sense but fail to capture a meaningful relationship between the predictors and the response.

### 2. **F-statistic**

- **F-statistic Role**:
  - The F-statistic tests the null hypothesis that all regression coefficients are zero (i.e., that the model has no explanatory power). A high F-statistic indicates that the predictors collectively have a significant effect on the response variable.

- **Possible Discrepancies**:
  - **High F-statistic with Low \(R^2\**: If the F-statistic is high but \(R^2\) is low, it could suggest that the model as a whole is statistically significant, but the proportion of explained variance is still relatively small. This might happen if the model improves the fit significantly compared to a baseline model, but the overall fit is still poor.
  - **Low F-statistic with High \(R^2\)**: If the F-statistic is low while \(R^2\) is high, this might be unusual, but it could occur in cases where the degrees of freedom or sample size are very small. The F-statistic might not be large enough to reach statistical significance, even if the model explains a lot of variance.

### 3. **Conflicting Metrics Scenario**

In practice, the metrics can sometimes provide conflicting indications:
- **Adding Predictors**: Adding more predictors usually increases \(R^2\) and might decrease RSE if the predictors are useful. However, if the additional predictors are irrelevant, \(R^2\) may be inflated and RSE might not decrease, leading to a misleading impression of model quality.
- **Model Overfitting**: A model with many predictors might show high \(R^2\) and low RSE on the training data but perform poorly on new data (high variance), which may not be apparent from these metrics alone.

### Summary

- **RSE** provides a direct measure of the average size of residuals and reflects how well the model fits the data in absolute terms.
- **\(R^2\)** indicates the proportion of the variance in the response variable that is explained by the model and is often used to assess the goodness of fit.
- **F-statistic** evaluates the overall significance of the model, testing whether the predictors, as a group, contribute significantly to explaining the response variable.

When interpreting these metrics, it's important to consider the context and potential limitations. For example, **\(R^2\)** alone may be misleading if used without considering the number of predictors or the potential for overfitting. The **F-statistic** helps assess model significance, but its value should be interpreted alongside other metrics and diagnostic tests to get a comprehensive understanding of model performance.

## Multiple Linear Regression

### 3.2.1 Estimating the Regression Coefficients

### 3.2.2 Some Important Questions

__Model fit, and relationship of predictor and response__
Is there a relationship between the response and predictors? 

1. Start with the F-statistic: 

If the overall model is significant (large F-statistic with a small p-value), this suggests that the predictors collectively have a significant relationship with the response.

2. Check R-squared: 

A high $R^2$ indicates that the model explains a substantial proportion of the variance in the response variable.

3. Review RSE: 

A low RSE indicates that the modelâ€™s predictions are close to the actual values.

4. Look at Individual t-statistics: 

Identify which predictors are significantly contributing to the model by checking the t-statistics and associated p-values. Predictors with significant t-statistics (small p-values) have a significant relationship with the response variable.

__Important Variables__

Individual t-statistics of variables. 

## Likelihood, AIC and BIC

### Likelihood: general definition

$$
L(\theta|Y, X) = \prod_{i = 1}^{n}f(Y_i|X_i, \theta)
$$

- $\theta$: parameters of the model; 

- $f(Y_i|X_i, \theta)$: probability density function of the observed data $Y_i$ given predictor $X_i$ and parameter $\theta$. 

### Likelihood of Standard Normal Distribution

__Standard Normal Distribution__

For a single observation $x_i$, the probability density function: 

$$
f(x_i) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
$$

$n$ independent observations $x_1, x_2, \dots, x_n$, the likelihood function $L$: 

$$
\begin{align}
L(x_1, x_2, \dots, x_n |\mu = 0, \sigma = 1) &= \prod_{i = 1}^{n}\frac{1}{\sqrt{2\pi}}e^{-\frac{x_i^2}{2}} \\
&=\bigg(\frac{1}{\sqrt{2\pi}}\bigg)^ne^{-\frac{1}{2}\sum_{i = 1}^{n}x_i^2}
\end{align}
$$

Likelihood: the product of the individual probabilities/probabilities densities. 

Based on the likelihood function above, two scenario further: 

- scenario 1: the n observations are close to the center of the distribution, 

- scenario 2: n observations are scattered. what is the difference of the likelihood of these two scenario

A Fishing Analogy(assuming the distribution): 

Fish Near the center

Fish scattered across the pond

### Likelihood of Linear regression: 

__Linear Regression with Normal Errors: __

$$
Y_i|X_i \sim \mathcal{N}(\beta_0 + \beta_1X_i, \sigma^2)
$$

__Probability Density function of $y_i$ given $x_i$__

$$
f(y_i |x_i, \beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\bigg(-\frac{(y_i - (\beta_0 + \beta_1x_i))^2}{2\sigma^2}\bigg)
$$

__Likelihood Function__: 

$$
L(\beta_0, \beta_1, \sigma^2|x_1, y_1, \dots, x_n, y_n) = \prod_{i = 1}^{n}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\bigg(-\frac{(y_i - (\beta_0 + \beta_1x_i))^2}{2\sigma^2}\bigg)
$$

__Log-Likelihood Function__

Summary

Likelihood Function: The likelihood function for linear regression quantifies how probable the observed data is given the parameters $\beta_0$, $\beta_1$, $\sigma^2$
â€‹
Log-Likelihood Function: Simplifies the calculation and is used to find parameter estimates by maximizing it.

MLE: Estimating $\beta_0$, $\beta_1$ using MLE is equivalent to performing ordinary least squares regression. The MLE for is the average of the squared residuals.
In summary, the likelihood function in linear regression helps us estimate the parameters of the model by maximizing the probability of observing the given data under the assumed linear relationship.

### AIC and BIC

Akaike Information Criterion (AIC) 

$$
\text{AIC} = 2k -2\log L
$$

k is the number of parameters in the model.

L is the maximum value of the likelihood function for the model.

Lower AIC values indicate a better-fitting model, taking into account the number of parameters. It balances goodness of fit and model complexity.


Bayesian Information Criterion (BIC)

$$
\text{BIC} = \log(n)\times k - 2\log L
$$

n is the number of observations.

k is the number of parameters in the model.

L is the maximum value of the likelihood function for the model.

Lower BIC values indicate a better-fitting model, but BIC penalizes model complexity more heavily compared to AIC, especially as the sample size n increases.

### Integrated metric for linear model evaluation

__Fit and Complexity__

R-squared and Adjusted R-squared provide a measure of how well the model fits the data. Use $R^2$ for initial insights and $R_{adj}^{2}$ for adjusting based on the number of predictors. 


MSE provides a measure of the average prediction error, useful for understanding the absolute fit of the model.


__Model Significance__

F-score tests whether the modelâ€™s predictors are jointly significant. High F-values suggest that the model explains a significant amount of variance.

__Model Selection__

AIC and BIC are used for model comparison. They help in selecting the best model among several by balancing fit and complexity. AIC is more forgiving with complexity, while BIC is more stringent, especially with large sample sizes.

## A flow chart of model diagnosis

                [Data Preparation]
                      â†“
                [Fit Linear Model]
                      â†“
          [Model Evaluation]
          /         |         \
   [R-squared]  [Adjusted R-squared]  [F-statistic]
         â†“              â†“                  â†“
    [Acceptable?]   [Acceptable?]     [Significant?]
      /  \                /  \                /  \
   No   Yes            No   Yes           No   Yes
   â†“     â†“             â†“     â†“            â†“      â†“
[Improve Model]   [MSE Check]        [Check MSE]
                           â†“                   â†“
                [Is MSE Acceptable?]
                        /  \
                    No    Yes
                    â†“        â†“
           [Improve Model] [AIC & BIC Check]
                                        â†“
                             [Check AIC & BIC]
                                       â†“
                     [Model Diagnostics]
                     /     |     |    \     \
[Outlier Detection] [Influential Points] [VIF] [Residuals Plot]
      â†“                       â†“                â†“         â†“
 [Are there outliers?] [Check Cook's Distance] [High VIF?] [Acceptable?]
       /  \                     /  \                /  \       /  \
     Yes  No                  Yes  No           Yes  No     No   Yes
     â†“     â†“                    â†“     â†“            â†“      â†“       â†“
[Investigate]  [Cross-Validation] [Address Multicollinearity]
                             â†“
                      [Is Model Satisfactory?]
                                             /    \
                                          No      Yes
                                          â†“        â†“
                                [Refine Model]  [Finalize Model]



## Other Considerations in the Regression Model

### Qualitative predictors with two or more levels

### Extension of the linear model: Removing the additive assumption: interaction with other variables to change the slope. 

### Extension of the linear model: Non-linear relationships. 

### Diamond example. 

$$
\begin{equation}
\text{price} = \beta_0 + \beta_1 \cdot \text{carat} + \gamma_1 \cdot \text{cut}_\text{Ideal} + \gamma_2 \cdot \text{cut}_\text{Premium} + \gamma_3 \cdot \text{cut}_\text{Good} + \gamma_4 \cdot \text{cut}_\text{Very Good} + \epsilon
\end{equation}
$$

$$
\begin{equation}
\text{price} = \beta_0 + \beta_1 \cdot \text{carat} 
+ \underbrace{\gamma_1 \cdot \text{cut}_\text{Ideal} + \gamma_2 \cdot \text{cut}_\text{Premium} + \gamma_3 \cdot \text{cut}_\text{Good} + \gamma_4 \cdot \text{cut}_\text{Very Good}}_{\text{Dummy Variables for Cut}} 
+ \underbrace{\delta_1 \cdot (\text{carat} \cdot \text{cut}_\text{Ideal}) + \delta_2 \cdot (\text{carat} \cdot \text{cut}_\text{Premium}) + \delta_3 \cdot (\text{carat} \cdot \text{cut}_\text{Good}) + \delta_4 \cdot (\text{carat} \cdot \text{cut}_\text{Very Good})}_{\text{Interaction Terms}} 
+ \epsilon
\end{equation}

$$

### Potential problems: 

### Potential problems: 1. Non-linearity

True function: $y = x^2 + 3x + 2$

Fit as linear: $y = 3x + 2$

Residual is quadratic. 

### Potential problems: 2. Correlation of error terms

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(MASS) # for mvrnorm()

set.seed(123) # For reproducibility

# Parameters
n <- 100         # Number of observations
rho <- 0.1    # Correlation coefficient for errors

# Generate X values
X <- rnorm(n)

# Create correlated errors
# Define a covariance matrix
cov_matrix <- matrix(rho, nrow=n, ncol=n)
diag(cov_matrix) <- 1  # Variances on the diagonal

# Generate errors from the multivariate normal distribution
errors <- mvrnorm(n=1, mu=rep(0, n), Sigma=cov_matrix)

# Define a linear model with correlated errors
beta0 <- 5
beta1 <- 2
Y <- beta0 + beta1 * X + errors

# Create a data frame
data <- data.frame(X = X, Y = Y, Errors = errors)

# Plot Y vs X
plot1 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  ggtitle("Scatter Plot of Y vs X") +
  theme_minimal()

# Plot Errors
plot2 <- ggplot(data, aes(x = seq_along(Errors), y = Errors)) +
  geom_line() +
  ggtitle("Plot of Errors") +
  theme_minimal()

# Display plots
library(gridExtra)
grid.arrange(plot1, plot2, nrow=2)

```

```{r}
library(ggplot2)
library(gridExtra)

# Define function to generate AR(1) errors
generate_AR1_errors <- function(n, rho) {
  errors <- numeric(n)
  errors[1] <- rnorm(1)
  for (i in 2:n) {
    errors[i] <- rho * errors[i - 1] + rnorm(1)
  }
  return(errors)
}

# Set parameters
n <- 100
rho <- 0.99  # High correlation

# Generate X values
X <- rnorm(n)

# Generate errors with AR(1) process
errors_high <- generate_AR1_errors(n, rho)

# Create dataset
beta0 <- 5
beta1 <- 2
Y_high <- beta0 + beta1 * X + errors_high
data_high <- data.frame(X = X, Y = Y_high, Errors = errors_high)
```

```{r}
model1 <- lm(Y_high~X, data = data_high)

summary(model1)
```

```{r}

# Plot Y vs X
plot1_high <- ggplot(data_high, aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  ggtitle("Scatter Plot of Y vs X (High Correlation Errors)") +
  theme_minimal()

# Plot Errors
plot2_high <- ggplot(data_high, aes(x = seq_along(Errors), y = Errors)) +
  geom_line() +
  ggtitle("Plot of Errors (High Correlation with AR(1))") +
  theme_minimal()

# Display plots
grid.arrange(plot1_high, plot2_high, nrow = 2)

```



```{r}
library(ggplot2)
library(gridExtra)

# Define function to generate AR(1) errors
generate_AR1_errors <- function(n, rho) {
  errors <- numeric(n)
  errors[1] <- rnorm(1)
  for (i in 2:n) {
    errors[i] <- rho * errors[i - 1] + rnorm(1)
  }
  return(errors)
}

# Set parameters
n <- 100
rho <- 0.05  # High correlation

# Generate X values
X <- rnorm(n)

# Generate errors with AR(1) process
errors_low <- generate_AR1_errors(n, rho)

# Create dataset
beta0 <- 5
beta1 <- 2
Y_low <- beta0 + beta1 * X + errors_low
data_low <- data.frame(X = X, Y = Y_low, Errors = errors_low)
```

```{r}
model2 <- lm(Y_low~X, data = data_low)

summary(model2)
```

```{r}

# Plot Y vs X
plot1_low <- ggplot(data_low, aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  ggtitle("Scatter Plot of Y vs X (High Correlation Errors)") +
  theme_minimal()

# Plot Errors
plot2_low <- ggplot(data_low, aes(x = seq_along(Errors), y = Errors)) +
  geom_line() +
  ggtitle("Plot of Errors (High Correlation with AR(1))") +
  theme_minimal()

# Display plots
grid.arrange(plot1_low, plot2_low, nrow = 2)
```



### Potential problems: 3. Non-constant variance of error terms(heteroscedasticity)

### Potential problems: 4. Outliers

Definition: Outliers are data points that have a large residual, meaning they deviate significantly from the fitted regression line. They are unusual in the response variable space.

### Potential problems: 5. High-leverage points

Definition: Leverage points are data points with extreme values in the predictor variable(s) space. They have a higher potential to influence the fit of the regression model because they are far from the center of the predictor variable distribution.

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)
library(broom)  # For tidy model summaries

# Generate example data
set.seed(123)
n <- 100
X <- rnorm(n)
Y <- 3 * X + rnorm(n)
# Add some extreme values (leverage points and outliers)
X[c(10, 20, 30)] <- c(5, -5, 6)
Y[c(10, 20, 30)] <- c(20, -20, 25)
data <- data.frame(X = X, Y = Y)

# Fit linear model
model <- lm(Y ~ X, data = data)

# Calculate leverage, residuals, and influence
data <- data %>%
  mutate(
    Leverage = hatvalues(model),
    Residual = residuals(model),
    StdResidual = rstandard(model),
    CookD = cooks.distance(model)
  )

# Identify points of interest
high_leverage <- data %>% filter(Leverage > (2 * (2 / n)))
outliers <- data %>% filter(abs(StdResidual) > 2)
influential <- data %>% filter(CookD > (4 / n))

# Plotting
p1 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  geom_point(data = high_leverage, aes(x = X, y = Y), color = "red", shape = 1, size = 3) +
  ggtitle("Data with High Leverage Points") +
  theme_minimal()

p2 <- ggplot(data, aes(x = X, y = Residual)) +
  geom_point() +
  geom_point(data = outliers, aes(x = X, y = Residual), color = "blue", shape = 1, size = 3) +
  ggtitle("Data with Outliers") +
  theme_minimal()

p3 <- ggplot(data, aes(x = X, y = CookD)) +
  geom_point() +
  geom_point(data = influential, aes(x = X, y = CookD), color = "purple", shape = 1, size = 3) +
  ggtitle("Data with Influential Points (Cook's Distance)") +
  theme_minimal()

# Display plots
library(gridExtra)
grid.arrange(p1, p2, p3, nrow = 3)

```

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)
library(broom)  # For tidy model summaries
library(gridExtra)  # For arranging multiple plots

# Generate example data
set.seed(123)
n <- 100
X <- rnorm(n)
Y <- 3 * X + rnorm(n)
# Add some extreme values (leverage points and outliers)
X[c(10, 20, 30, 40)] <- c(5, -5, 6, 4.5)
Y[c(10, 20, 30, 40)] <- c(20, -20, 25, 15)
data <- data.frame(X = X, Y = Y)

# Fit linear model
model <- lm(Y ~ X, data = data)

# Calculate leverage, residuals, and influence
data <- data %>%
  mutate(
    Leverage = hatvalues(model),
    Residual = residuals(model),
    StdResidual = rstandard(model),
    CookD = cooks.distance(model)
  )

# Identify points of interest
high_leverage <- data %>% filter(Leverage > (2 * (2 / n)))
outliers <- data %>% filter(abs(StdResidual) > 2)
influential <- data %>% filter(CookD > (4 / n))

# Identify points that are both leverage and outliers
leverage_outliers <- high_leverage %>% filter(abs(StdResidual) > 2)

# Plotting
p1 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  geom_point(data = high_leverage, aes(x = X, y = Y), color = "red", shape = 1, size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  ggtitle("Data with High Leverage Points") +
  theme_minimal()

p2 <- ggplot(data, aes(x = X, y = Residual)) +
  geom_point() +
  geom_point(data = outliers, aes(x = X, y = Residual), color = "blue", shape = 1, size = 3) +
  geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "black") +
  ggtitle("Data with Outliers") +
  theme_minimal()

p3 <- ggplot(data, aes(x = X, y = CookD)) +
  geom_point() +
  geom_point(data = influential, aes(x = X, y = CookD), color = "purple", shape = 1, size = 3) +
  geom_hline(yintercept = 4 / n, linetype = "dashed", color = "black") +
  ggtitle("Data with Influential Points (Cook's Distance)") +
  theme_minimal()

p4 <- ggplot(data, aes(x = X, y = Y)) +
  geom_point() +
  geom_point(data = leverage_outliers, aes(x = X, y = Y), color = "orange", shape = 1, size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  ggtitle("Data with Leverage and Outlier Points") +
  theme_minimal()

# Display plots
grid.arrange(p1, p2, p3, p4, nrow = 2)

```

Formula of Cook's Distance

Linear regression: 

$$
\underset{n \times 1}y = \underset{n \times p} {X} \underset{p \times 1}\beta + \underset{n \times 1}\epsilon
$$

Least square estimator: 

$$
\text{b} = (\text{X}^{T}\text{X})^{-1}\text{X}^{T}\text{y}
$$

$$
\hat{\text{y}} = \text{X}\text{b} = (\text{X}^{T}\text{X})^{-1}\text{X}^{T}\text{y} = \text{H}\text{y}
$$

Cook's distance $D_i$ of the observation $i$ (for $i = 1, \dots, n$) is defined as the sum of all the changes in the regression model when observation $i$ is removed from the observations. 

$$
D_i = \frac{\sum_{j = 1}^{n}(\hat{y_j} - \hat{y}_{j(i)})^2}{ps^2}
$$

where: 

$\hat{y}_j$: fitted value including the $i$ observation; 

$\hat{y}_{j(i)}$: fitted value excluding the $i$th observation; 

$p$: the rank of the design matrix

$s^2$: $\frac{\text{e}^{\text{T}}\text{e}}{n - p}$



$$
D_i = \frac{(e_i^2/p) \cdot h_{ii}}{(1-h_{ii})^2}
$$



```{r}
X <- matrix(c(1, 2, 5, 1, 3, 6, 1, 4, 7), nrow = 3, byrow = TRUE)

X_transpose <- t(X)

X; X_transpose * X
```


6. Collinearity

VIF: 

$$
\text{VIF}(\hat{\beta}_{j}) = \frac{1}{1 - R_{X_j|X_{-j}}^{2}}
$$

## The Marketing Plan

## Comparison of Linear Regression with K-Nearest Neighbors

## Linear Regression for Later Machine Learning

Linear regression is classic, what are the learnings from linear regression that can be jump-off points that are helpful to late state-of-art machine learning approaches

- Model Interpretability

- Regularization

- Assumpution and assumption testing tools

- Model specification and feature engineering

- Model evaluation: accuracy, F1-Score, AUC_ROC

- Cross-validation

- Bias-Variance trade-off


## Concepts Summary

## Lab: Linear Regression

## Exercises. 